{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "622d3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import TextVectorization, Input, Embedding, LSTM, Dense, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "2a2e5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the CSV has been read into `dataframe`\n",
    "dataframe = pd.read_csv('pro_corpus.csv')\n",
    "aave_texts = dataframe['AAVE'].str.lower().tolist()\n",
    "sae_texts = [\"[start] \" + text + \" [end]\" for text in dataframe['SAE'].str.lower().tolist()]\n",
    "\n",
    "aave_train, aave_test, sae_train, sae_test = train_test_split(\n",
    "    aave_texts, sae_texts, test_size=0.2, random_state=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "980530fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 20000\n",
    "sequence_length = 30\n",
    "\n",
    "aave_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_vocab_size, output_sequence_length=sequence_length)\n",
    "sae_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_vocab_size, output_sequence_length=sequence_length + 1)  # +1 for [start]/[end] tokens\n",
    "\n",
    "aave_vectorization.adapt(aave_train)\n",
    "sae_vectorization.adapt(sae_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6afd2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(position, d_model):\n",
    "        angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                np.arange(d_model)[np.newaxis, :],\n",
    "                                d_model)\n",
    "\n",
    "        # Apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # Apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a467d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        \n",
    "        self.dense = Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # Correctly call the scaled_dot_product_attention function\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_v, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_v, d_model)\n",
    "            \n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2bf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e55a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732d4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "34288cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c808fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "198cb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "a18dd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ccff4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b903dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(pe_target, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # Adding embedding and positional encoding.\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            # Store attention weights, could be useful for visualization or analysis\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6bbd144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        print(\"Inputs dict keys:\", inputs.keys())\n",
    "        inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
    "        print(\"inp shape:\", inp.shape)\n",
    "        print(\"tar shape:\", tar.shape)\n",
    "        # Proceed with masks creation and model operations\n",
    "\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "        \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        print(\"inp shape:\", inp.shape)  # Debugging line\n",
    "        print(\"tar shape:\", tar.shape) \n",
    "        # Encoder padding mask for masking out the padding tokens in the encoder input\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Decoder padding mask for the second multi-head attention mechanism in the decoder\n",
    "        # This is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Look-ahead mask and decoder target padding mask combined to mask out tokens in the decoder that should not be seen yet\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)  # Combine masks to ensure both conditions are applied\n",
    "\n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2c9ef640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Skipping other necessary components (e.g., EncoderLayer, DecoderLayer, Encoder, Decoder) for brevity\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "# Define other necessary components (e.g., Encoder, Decoder) and training process as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "5ccc9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming aave_vectorization and sae_vectorization have been adapted on the respective datasets\n",
    "def make_dataset(aave, sae):\n",
    "    aave_ds = aave_vectorization(aave)\n",
    "    sae_ds = sae_vectorization(sae)\n",
    "    # Decoder inputs use the [:, :-1] slices of sae_ds, and the targets are the [:, 1:] slices\n",
    "    input_ds = {\"inputs\": aave_ds, \"dec_inputs\": sae_ds[:, :-1]}\n",
    "    target_ds = sae_ds[:, 1:]  # Targets are offset by 1 to predict the next token\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ds, target_ds)).batch(64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(aave_train, sae_train)\n",
    "val_ds = make_dataset(aave_test, sae_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8d7c6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_loss_function(real, pred):\n",
    "#     print(\"Shape of real (labels):\", real.shape)\n",
    "#     print(\"Shape of pred (logits):\", pred.shape)\n",
    "#     loss_ = loss_object(real, pred)\n",
    "#     return loss_\n",
    "\n",
    "# # Use the debug loss function for model compilation to inspect shapes\n",
    "# transformer.compile(optimizer=optimizer, loss=debug_loss_function, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "706f09ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of aave_ds after vectorization: (4630, 30)\n",
      "Shape of sae_ds after vectorization: (4630, 31)\n",
      "Shape of decoder inputs: (4630, 30)\n",
      "Shape of targets: (4630, 30)\n",
      "Shape of aave_ds after vectorization: (1158, 30)\n",
      "Shape of sae_ds after vectorization: (1158, 31)\n",
      "Shape of decoder inputs: (1158, 30)\n",
      "Shape of targets: (1158, 30)\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(aave, sae):\n",
    "    aave_ds = aave_vectorization(aave)\n",
    "    sae_ds = sae_vectorization(sae)\n",
    "    print(\"Shape of aave_ds after vectorization:\", aave_ds.shape)\n",
    "    print(\"Shape of sae_ds after vectorization:\", sae_ds.shape)\n",
    "\n",
    "    # Decoder inputs use the [:, :-1] slices of sae_ds, and the targets are the [:, 1:] slices\n",
    "    input_ds = {\"inputs\": aave_ds, \"dec_inputs\": sae_ds[:, :-1]}\n",
    "    target_ds = sae_ds[:, 1:]  # Targets are offset by 1 to predict the next token\n",
    "    print(\"Shape of decoder inputs:\", sae_ds[:, :-1].shape)\n",
    "    print(\"Shape of targets:\", sae_ds[:, 1:].shape)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ds, target_ds)).batch(64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = make_dataset(aave_train, sae_train)\n",
    "val_ds = make_dataset(aave_test, sae_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "85cddf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    # size: scalar, size of the mask (seq_len, seq_len)\n",
    "\n",
    "    # Create a lower triangular matrix filled with ones\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "81487638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return tf.expand_dims(tf.expand_dims(seq, 1), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "09aaaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "# Hyperparameters for the Transformer\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "input_vocab_size = aave_vectorization.vocabulary_size() + 2  # +2 for start/end tokens\n",
    "target_vocab_size = sae_vectorization.vocabulary_size() + 2  # +2 for start/end tokens\n",
    "pe_input = max([len(sentence.split()) for sentence in aave_texts])  # or a fixed number like 1000\n",
    "pe_target = max([len(sentence.split()) for sentence in sae_texts])  # or a fixed number like 1000\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                          input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n",
    "                          pe_input=pe_input, pe_target=pe_target, rate=dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b01ce275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 30)\n",
      "Target shape: (64, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 20:02:54.027471: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in train_ds.take(1):\n",
    "    print(\"Input shape:\", example_input_batch['inputs'].shape)\n",
    "    print(\"Target shape:\", example_target_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8d9321d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "1abc81e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 20:09:07.664558: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-02-06 20:09:07.720003: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 44s 449ms/step - loss: 7.9891 - accuracy: 0.0033 - val_loss: 7.7717 - val_accuracy: 0.0332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a36ae8b80>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "# Define the checkpoint path and the checkpoint manager.\n",
    "# This saves checkpoints to disk.\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# If a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "93b908b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 20:01:35.083395: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_33808/2318031034.py\", line 4, in train_step  *\n        predictions, _ = transformer([inp, tar[:, :-1]], training=True)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filethr9xh_d.py\", line 10, in tf__call\n        ag__.ld(print)('Inputs dict keys:', ag__.converted_call(ag__.ld(inputs).keys, (), None, fscope))\n\n    AttributeError: Exception encountered when calling layer \"transformer_16\" (type Transformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_33808/2912783195.py\", line 11, in call  *\n            print(\"Inputs dict keys:\", inputs.keys())\n    \n        AttributeError: 'list' object has no attribute 'keys'\n    \n    \n    Call arguments received by layer \"transformer_16\" (type Transformer):\n      • inputs=['tf.Tensor(shape=(64, 30), dtype=int64)', 'tf.Tensor(shape=(64, 29), dtype=int64)']\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [289]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, tar)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_ds):\n\u001b[0;32m---> 13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdec_inputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file05l7nja3.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(inp, tar)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     (predictions, _) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss_function), (ag__\u001b[38;5;241m.\u001b[39mld(tar)[:, \u001b[38;5;241m1\u001b[39m:], ag__\u001b[38;5;241m.\u001b[39mld(predictions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(transformer)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filethr9xh_d.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInputs dict keys:\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope))\n\u001b[1;32m     11\u001b[0m (inp, tar) \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdec_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minp shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(inp)\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_33808/2318031034.py\", line 4, in train_step  *\n        predictions, _ = transformer([inp, tar[:, :-1]], training=True)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filethr9xh_d.py\", line 10, in tf__call\n        ag__.ld(print)('Inputs dict keys:', ag__.converted_call(ag__.ld(inputs).keys, (), None, fscope))\n\n    AttributeError: Exception encountered when calling layer \"transformer_16\" (type Transformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_33808/2912783195.py\", line 11, in call  *\n            print(\"Inputs dict keys:\", inputs.keys())\n    \n        AttributeError: 'list' object has no attribute 'keys'\n    \n    \n    Call arguments received by layer \"transformer_16\" (type Transformer):\n      • inputs=['tf.Tensor(shape=(64, 30), dtype=int64)', 'tf.Tensor(shape=(64, 29), dtype=int64)']\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar[:, :-1]], training=True)\n",
    "        loss = loss_function(tar[:, 1:], predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (batch, (inp, tar)) in enumerate(train_ds):\n",
    "        loss = train_step(inp['inputs'], inp['dec_inputs'])\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {batch} Loss {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "449e12f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch keys: dict_keys(['inputs', 'dec_inputs'])\n",
      "Input shape: (64, 30)\n",
      "Target shape: (64, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 16:30:49.310522: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in train_ds.take(1):\n",
    "    print(\"Input batch keys:\", example_input_batch.keys())\n",
    "    print(\"Input shape:\", example_input_batch['inputs'].shape)\n",
    "    print(\"Target shape:\", example_target_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "599ffed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding mask: tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 1. 1.]]]], shape=(2, 1, 1, 5), dtype=float32)\n",
      "Look-ahead mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_seq = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0]], dtype=tf.int32)\n",
    "print(\"Padding mask:\", create_padding_mask(test_seq))\n",
    "print(\"Look-ahead mask:\", create_look_ahead_mask(size=5))  # Adjust size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01196348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "57409036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, training):\n",
    "    print(\"Inputs dict keys:\", inputs.keys())\n",
    "    inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
    "    print(\"inp shape:\", inp.shape)\n",
    "    print(\"tar shape:\", tar.shape)\n",
    "    # Proceed with masks creation and model operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c352b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d5d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c67191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "dataframe = pd.read_csv('pro_corpus.csv')\n",
    "assert 'AAVE' in dataframe.columns and 'SAE' in dataframe.columns\n",
    "\n",
    "# Preparing the dataset\n",
    "aave_texts = dataframe['AAVE'].str.lower().tolist()\n",
    "sae_texts = dataframe['SAE'].str.lower().tolist()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "aave_train, aave_test, sae_train, sae_test = train_test_split(\n",
    "    aave_texts, sae_texts, test_size=0.2, random_state=21)\n",
    "\n",
    "# Convert the train and test data into TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'aave': aave_train,\n",
    "    'sae': sae_train\n",
    "})\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'aave': aave_test,\n",
    "    'sae': sae_test\n",
    "})\n",
    "\n",
    "BUFFER_SIZE = len(aave_train)  # Use the size of the train dataset\n",
    "\n",
    "# Adjust batch sizes\n",
    "train_batch_size = 16\n",
    "test_batch_size = 4\n",
    "\n",
    "# Shuffle and batch the train dataset\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(train_batch_size, drop_remainder=True)\n",
    "\n",
    "# Batch the test dataset\n",
    "test_dataset = test_dataset.batch(test_batch_size, drop_remainder=True)\n",
    "\n",
    "# Text Vectorization\n",
    "aave_vectorization = TextVectorization(output_mode='int', output_sequence_length=30)\n",
    "sae_vectorization = TextVectorization(output_mode='int', output_sequence_length=30)\n",
    "\n",
    "aave_texts = train_dataset.map(lambda x: x['aave'])\n",
    "sae_texts = train_dataset.map(lambda x: x['sae'])\n",
    "\n",
    "aave_vectorization.adapt(aave_texts)\n",
    "sae_vectorization.adapt(sae_texts)\n",
    "\n",
    "aave_vocab_size = len(aave_vectorization.get_vocabulary())\n",
    "sae_vocab_size = len(sae_vectorization.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f52b943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embedding_dim = 256\n",
    "units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d6f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8f776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5db578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, None, 256)    740096      ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 [(None, None, 1024)  5246976     ['embedding_8[0][0]']            \n",
      "                                , (None, 1024),                                                   \n",
      "                                 (None, 1024)]                                                    \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, None, 256)    800000      ['decoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 [(None, None, 1024)  8392704     ['lstm_16[0][0]']                \n",
      "                                , (None, 1024),                                                   \n",
      "                                 (None, 1024)]                                                    \n",
      "                                                                                                  \n",
      " lstm_18 (LSTM)                 [(None, None, 1024)  5246976     ['embedding_9[0][0]',            \n",
      "                                , (None, 1024),                   'lstm_17[0][1]',                \n",
      "                                 (None, 1024)]                    'lstm_17[0][2]']                \n",
      "                                                                                                  \n",
      " lstm_19 (LSTM)                 [(None, None, 1024)  8392704     ['lstm_18[0][0]']                \n",
      "                                , (None, 1024),                                                   \n",
      "                                 (None, 1024)]                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_layer_4 (  (None, None, 256)   8658176     ['lstm_19[0][0]',                \n",
      " MultiHeadAttentionLayer)                                         'lstm_17[0][0]',                \n",
      "                                                                  'lstm_17[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, None, 1280)   0           ['lstm_19[0][0]',                \n",
      "                                                                  'multi_head_attention_layer_4[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, None, 3125)   4003125     ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,480,757\n",
      "Trainable params: 41,480,757\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "# Building the Enhanced Model\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
    "encoder_embedding = Embedding(input_dim=aave_vocab_size, output_dim=embedding_dim)(encoder_input)\n",
    "encoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_lstm2 = LSTM(units, return_sequences=True, return_state=True)\n",
    "encoder_outputs2, state_h2, state_c2 = encoder_lstm2(encoder_outputs)\n",
    "encoder_state = [state_h2, state_c2]\n",
    "\n",
    "\n",
    "# MultiHeadAttention Parameters\n",
    "num_heads = 8  # Number of attention heads\n",
    "\n",
    "# MultiHeadAttention Layer\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dense_output = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, query, value, key, attention_mask=None):\n",
    "        attention_output = self.multi_head_attention(query=query, value=value, key=key, attention_mask=attention_mask)\n",
    "        output = self.dense_output(attention_output)\n",
    "        return output\n",
    "\n",
    "# Modify the Decoder to include MultiHeadAttention\n",
    "# Decoder with MultiHeadAttention\n",
    "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
    "decoder_embedding = Embedding(input_dim=sae_vocab_size, output_dim=embedding_dim)(decoder_input)\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_state)\n",
    "decoder_lstm2 = LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_lstm_output2, _, _ = decoder_lstm2(decoder_lstm_output)\n",
    "\n",
    "\n",
    "\n",
    "# Applying MultiHeadAttention\n",
    "multi_head_attention = MultiHeadAttentionLayer(d_model=embedding_dim, num_heads=num_heads)\n",
    "attention_output = multi_head_attention(decoder_lstm_output2, encoder_outputs2, encoder_outputs2)\n",
    "\n",
    "# Concatenation\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_lstm_output2, attention_output])\n",
    "\n",
    "# Output Layer\n",
    "decoder_dense = Dense(sae_vocab_size, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model_M4 = Model([encoder_input, decoder_input], decoder_output)\n",
    "model_M4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_M4.summary()\n",
    "\n",
    "# The rest of the training and evaluation process remains similar to your original model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e46aefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(batch):\n",
    "    input_text = batch['aave']\n",
    "    target_text = batch['sae']\n",
    "\n",
    "    input_data = aave_vectorization(input_text)\n",
    "    target_data = sae_vectorization(target_text)\n",
    "\n",
    "    # Ensure all sequences in the batch have the same length\n",
    "    sequence_length = 30\n",
    "    input_data = tf.ensure_shape(input_data, [None, sequence_length])\n",
    "    target_data = tf.ensure_shape(target_data, [None, sequence_length])\n",
    "\n",
    "    return {'encoder_input': input_data, 'decoder_input_M4': target_data[:, :-1]}, target_data[:, 1:]\n",
    "\n",
    "# Apply the function to each item in the dataset\n",
    "train_dataset = train_dataset.map(split_input_target).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(split_input_target).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89ad8a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 183, in assert_input_compatibility\n        raise ValueError(f'Missing data for input \"{name}\". '\n\n    ValueError: Missing data for input \"decoder_input\". You passed a data dictionary with keys ['encoder_input', 'decoder_input_M4']. Expected the following keys: ['encoder_input', 'decoder_input']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m early_stopping_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     14\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model with validation split and callbacks\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_M4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file60zqgcx0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/N/soft/sles15/python/gnu/3.10.5/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 183, in assert_input_compatibility\n        raise ValueError(f'Missing data for input \"{name}\". '\n\n    ValueError: Missing data for input \"decoder_input\". You passed a data dictionary with keys ['encoder_input', 'decoder_input_M4']. Expected the following keys: ['encoder_input', 'decoder_input']\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "\n",
    "# Callbacks for Early Stopping and Model Checkpoint\n",
    "checkpoint_filepath = '/N/u/saswar/Carbonate/AAVE/'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    verbose=1)\n",
    "\n",
    "# Train the model with validation split and callbacks\n",
    "history = model_M4.fit(train_dataset, epochs=epochs, validation_data=test_dataset, callbacks=[early_stopping_callback, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f5d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
