{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c01228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import TextVectorization, Input, Embedding, LSTM, Dense, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f748b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the CSV has been read into `dataframe`\n",
    "dataframe = pd.read_csv('pro_corpus.csv')\n",
    "aave_texts = dataframe['AAVE'].str.lower().tolist()\n",
    "sae_texts = [\"[start] \" + text + \" [end]\" for text in dataframe['SAE'].str.lower().tolist()]\n",
    "\n",
    "aave_train, aave_test, sae_train, sae_test = train_test_split(\n",
    "    aave_texts, sae_texts, test_size=0.2, random_state=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6deb58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 20000\n",
    "sequence_length = 30\n",
    "\n",
    "aave_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_vocab_size, output_sequence_length=sequence_length)\n",
    "sae_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_vocab_size, output_sequence_length=sequence_length + 1)  # +1 for [start]/[end] tokens\n",
    "\n",
    "aave_vectorization.adapt(aave_train)\n",
    "sae_vectorization.adapt(sae_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3007d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7810b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f21396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db9a51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f77a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e16106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c31ce8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(pe_target, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # Adding embedding and positional encoding.\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            # Store attention weights, could be useful for visualization or analysis\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f8a7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Skipping other necessary components (e.g., EncoderLayer, DecoderLayer, Encoder, Decoder) for brevity\n",
    "\n",
    "# class Transformer(tf.keras.Model):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "#         self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "#         self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "#     def call(self, inputs, training):\n",
    "#         inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
    "\n",
    "#         enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "#         enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "#         dec_output, attention_weights = self.decoder(tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "\n",
    "#         final_output = self.final_layer(dec_output)\n",
    "\n",
    "#         return final_output\n",
    "\n",
    "#     def create_masks(self, inp, tar):\n",
    "#         enc_padding_mask = create_padding_mask(inp)\n",
    "#         dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "#         look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "#         dec_target_padding_mask = create_padding_mask(tar)\n",
    "#         combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "#         return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "# # Define other necessary components (e.g., Encoder, Decoder) and training process as needed\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Split inputs\n",
    "        inp, tar = inputs['inputs'], inputs['dec_inputs']\n",
    "        \n",
    "        # Call the create_masks function here\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        # Encoder output\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        # Decoder output\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Create padding mask for input\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "        \n",
    "        # Create look-ahead mask and padding mask for target\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        \n",
    "        # Decoder padding mask for second attention block in decoder\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "        \n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45576ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming aave_vectorization and sae_vectorization have been adapted on the respective datasets\n",
    "def make_dataset(aave, sae):\n",
    "    aave_ds = aave_vectorization(aave)\n",
    "    sae_ds = sae_vectorization(sae)\n",
    "    # Decoder inputs use the [:, :-1] slices of sae_ds, and the targets are the [:, 1:] slices\n",
    "    input_ds = {\"inputs\": aave_ds, \"dec_inputs\": sae_ds[:, :-1]}\n",
    "    target_ds = sae_ds[:, 1:]  # Targets are offset by 1 to predict the next token\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ds, target_ds)).batch(64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(aave_train, sae_train)\n",
    "val_ds = make_dataset(aave_test, sae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2f52e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "# Hyperparameters for the Transformer\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "input_vocab_size = aave_vectorization.vocabulary_size() + 2  # +2 for start/end tokens\n",
    "target_vocab_size = sae_vectorization.vocabulary_size() + 2  # +2 for start/end tokens\n",
    "pe_input = max([len(sentence.split()) for sentence in aave_texts])  # or a fixed number like 1000\n",
    "pe_target = max([len(sentence.split()) for sentence in sae_texts])  # or a fixed number like 1000\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                          input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n",
    "                          pe_input=pe_input, pe_target=pe_target, rate=dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdcda3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bdc7700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 53s 510ms/step - loss: 7.9782 - accuracy: 0.0100 - val_loss: 7.7753 - val_accuracy: 0.0333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9570cae90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "# Define the checkpoint path and the checkpoint manager.\n",
    "# This saves checkpoints to disk.\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# If a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b603d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, transformer, inp_lang_vectorizer, tar_lang_vectorizer):\n",
    "    # Preprocess and tokenize the input sentence\n",
    "    sentence = tf.convert_to_tensor([sentence])\n",
    "    sentence = inp_lang_vectorizer(sentence)\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # Assuming start token is 2 and end token is 3 for illustration; adjust according to your setup\n",
    "    start, end = 2, 3\n",
    "    output = tf.convert_to_tensor([start])\n",
    "    output = tf.expand_dims(output, 0)\n",
    "\n",
    "    for i in range(40):  # Assuming a maximum length of 40 tokens\n",
    "        predictions = transformer({'inputs': encoder_input, 'dec_inputs': output}, training=False)\n",
    "\n",
    "        # Select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # Concatenate the predicted_id to the output\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        # Check if the last predicted word is equal to the end token\n",
    "        if predicted_id.numpy()[0][0] == end:\n",
    "            break\n",
    "\n",
    "    output = tf.squeeze(output, axis=0).numpy()\n",
    "\n",
    "    # Convert sequence of IDs to text\n",
    "    vocabulary = tar_lang_vectorizer.get_vocabulary()\n",
    "    predicted_sentence = ' '.join([vocabulary[i] for i in output if i < len(vocabulary) and i != start])\n",
    "\n",
    "    # Optionally, strip the part after the end token if it was included\n",
    "    end_token_index = vocabulary.index('[end]') if '[end]' in vocabulary else -1\n",
    "    if end_token_index != -1:\n",
    "        predicted_sentence = predicted_sentence.split('[end]')[0]\n",
    "\n",
    "    return predicted_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34b75941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def calculate_bleu_score(input_texts, target_texts, transformer, inp_lang_vectorizer, tar_lang_vectorizer):\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    for i, sentence in enumerate(input_texts):\n",
    "        target = target_texts[i][len(\"[start] \"):-len(\" [end]\")]  # Remove the start and end tokens from the target\n",
    "        translation = translate(sentence, transformer, inp_lang_vectorizer, tar_lang_vectorizer)\n",
    "        \n",
    "        references.append([target.split(' ')])  # BLEU references need to be tokenized and wrapped in a list\n",
    "        candidates.append(translation.split(' '))\n",
    "\n",
    "    bleu_score = corpus_bleu(references, candidates)\n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ae834e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on test set: 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "bleu_score = calculate_bleu_score(aave_test[:20], sae_test[:20], transformer, aave_vectorization, sae_vectorization)  # Use a slice for quick testing\n",
    "print(f\"BLEU score on test set: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e9e22ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original AAVE Sentence: so, i would ask you to like start at the beginning and tell me about kind of\n",
      "Expected SAE Translation: so, i would like to ask you to start at the beginning and tell me about \n",
      "Model Translation: end\n",
      "--------------------------------------------------\n",
      "Original AAVE Sentence: had such bad times with teachers. um, what kind of teacher are you gonna be?\n",
      "Expected SAE Translation: had such bad times with teachers. um, what kind of teacher are you going to be?\n",
      "Model Translation: end\n",
      "--------------------------------------------------\n",
      "Original AAVE Sentence: yeah.\n",
      "Expected SAE Translation: yes.\n",
      "Model Translation: end\n",
      "--------------------------------------------------\n",
      "Original AAVE Sentence: [gotcha.]\n",
      "Expected SAE Translation: got you. \n",
      "Model Translation: end\n",
      "--------------------------------------------------\n",
      "Original AAVE Sentence: we have /different/ um,\n",
      "Expected SAE Translation: x\n",
      "Model Translation: end\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_translations(input_texts, target_texts, transformer, inp_lang_vectorizer, tar_lang_vectorizer, num_examples=5):\n",
    "    for i in range(num_examples):\n",
    "        sentence = input_texts[i]\n",
    "        expected_translation = target_texts[i][len(\"[start] \"):-len(\" [end]\")]  # Assuming the target_texts are wrapped with [start] and [end] tokens\n",
    "        \n",
    "        # Translate the sentence\n",
    "        translation = translate(sentence, transformer, inp_lang_vectorizer, tar_lang_vectorizer)\n",
    "        \n",
    "        print(f\"Original AAVE Sentence: {sentence}\")\n",
    "        print(f\"Expected SAE Translation: {expected_translation}\")\n",
    "        print(f\"Model Translation: {translation}\")\n",
    "        print(\"-\" * 50)  # Separator\n",
    "\n",
    "# Assuming `aave_test` and `sae_test` are your test datasets\n",
    "print_translations(aave_test, sae_test, transformer, aave_vectorization, sae_vectorization, num_examples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba495e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
